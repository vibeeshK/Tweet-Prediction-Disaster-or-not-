{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import words\n",
    "word_list = words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=r'C:\\Users\\Vibeesh\\Desktop\\kaggle data\\Tweets Nlp\\train.csv'\n",
    "traindata=pd.read_csv(filepath)\n",
    "filepaths=r'C:\\Users\\Vibeesh\\Desktop\\kaggle data\\Tweets Nlp\\test.csv'\n",
    "testdata=pd.read_csv(filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "traindata2=traindata\n",
    "testdata2=testdata\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "added_df = pd.concat([traindata,testdata])\n",
    "len(added_df['keyword'])\n",
    "\n",
    "\n",
    "textlist=added_df['text'].tolist()\n",
    "keywordlist=added_df['keyword'].tolist()\n",
    "targetlist=added_df['target'].tolist()\n",
    "\n",
    "len(textlist)\n",
    "\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "sp = spacy.load('en_core_web_sm')\n",
    "all_stopwords = sp.Defaults.stop_words\n",
    "textlist2=[]\n",
    "for i1 in textlist:\n",
    "    i1 = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', i1, flags=re.MULTILINE)\n",
    "    i1 = re.sub('[0-9]', '', i1)\n",
    "    i1 = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', i1, flags=re.MULTILINE)\n",
    "    i1=re.sub(r'[^\\w\\s]','',i1)\n",
    "    i1=re.sub(r'\\W', ' ', i1) \n",
    "    i1=i1.lower()\n",
    "    i2 = word_tokenize(i1)\n",
    "    i2= [word for word in i2 if not word in all_stopwords]\n",
    "  #  i2= [word for word in i2 if not word in word_list]\n",
    "   # word_list\n",
    "    i2 = [w for w in i2 if not w in stop_words]\n",
    "    i3=' '.join(i2)\n",
    "    textlist2.append(i3)\n",
    "\n",
    "\n",
    "\n",
    "#Now we Lemmetize the text\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "textlist3=[]\n",
    "for i4 in textlist2:\n",
    "    i4=lemmatizer.lemmatize(i4)\n",
    "    textlist3.append(i4)\n",
    "len(textlist3)\n",
    "\n",
    "\n",
    "added_df=added_df.drop('text',axis=1)\n",
    "added_df['text']=textlist3\n",
    "cv=CountVectorizer()\n",
    "added_df=cv.fit_transform(added_df['text'])\n",
    "added_df=pd.DataFrame(added_df.todense())\n",
    "\n",
    "#addedcv=addedcv.drop('keyword',axis=1)\n",
    "added_df['keywords']=keywordlist\n",
    "added_df['target']=targetlist\n",
    "\n",
    "added_df['keywords'] = added_df['keywords'].replace(np.nan, 'keywords', regex=True)\n",
    "#We need to change the names of the variables in FireplaceQu in order to perform OneHotENcoding\n",
    "added_df['keywords'] = 'keywords-' + added_df['keywords'].astype(str)\n",
    "added_df.head()\n",
    "\n",
    "#This column contains categorical variables and must be converted to numerical form using One Hot Encoding-\n",
    "one_hot = pd.get_dummies(added_df['keywords'])\n",
    "# Drop column Product as it is now encoded\n",
    "added_df = added_df.drop('keywords',axis = 1)\n",
    "# Join the encoded testdata\n",
    "added_df = added_df.join(one_hot)\n",
    "added_df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "added_df = added_df.replace(np.nan, 0, regex=True)\n",
    "added_df = added_df.reset_index()\n",
    "training=added_df.head(7613)\n",
    "testing=added_df.iloc[7613:]\n",
    "\n",
    "y=training['target']\n",
    "x=training.drop('target',axis=1)\n",
    "testing=testing.drop('target',axis=1)\n",
    "\n",
    "\n",
    "###We use only the keyword and text column because only they are mostly used to determine the\n",
    "\n",
    "#Logistic Regression\n",
    "#df = df.reset_index()\n",
    "LogisticRegressor = LogisticRegression(max_iter=10000)\n",
    "LogisticRegressor.fit(x, y)\n",
    "Prediction = LogisticRegressor.predict(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionlist=Prediction.tolist()\n",
    "Passengerid=testdata2['id'].tolist() \n",
    "output=pd.DataFrame(list(zip(Passengerid, predictionlist)),\n",
    "              columns=['id','target'])\n",
    "output.head()\n",
    "                    \n",
    "output.to_csv('my_submission(twitterdisaster)1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
